{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2YD0lFtfgMj"
   },
   "source": [
    "***TOKENIZATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aruSoi4Odonr",
    "outputId": "34213d9b-634e-4031-e58b-3f7779a44ef3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "text=\"\"\"Artificial Intelligence (AI) is performing industries worldwide. With its applications in healthcare, finance, and education, AI provides innovative solutions.\"\"\"\n",
    "\n",
    "# Sentence and word tokenization\n",
    "def tokenize_text(text):\n",
    "    print(\"--- Tokenization ---\")\n",
    "\n",
    "\n",
    "    # Uses the sent_tokenize function from the nltk.tokenize module to split\n",
    "    # The input text into a list of sentences.\n",
    "    print(\"Sentence Tokenizaation:\")\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(sentences)\n",
    "\n",
    "    # Uses the word_tokenize function from the nltk.tokenize module to split\n",
    "    # Input text into a list of words (or tokens).\n",
    "    print(\"\\nWord Tokenization:\")\n",
    "    words = word_tokenize(text)\n",
    "    print(words)\n",
    "\n",
    "\n",
    "    # Calls the tokenize_text function with the text variable as input\n",
    "    tokenize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf412eX0fnBD"
   },
   "source": [
    "***STEMMING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8CMiyBmeB_p",
    "outputId": "a2cb0102-4a50-4703-8e94-a181cf0d30bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming ---\n",
      "Stemmed words:\n",
      "['artifici', 'intellig', 'is', 'transform', 'industri', 'worldwid', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
    "tokenlist = word_tokenize(text)\n",
    "\n",
    "# Perform stemming\n",
    "def stem_text(tokenlist):\n",
    "\n",
    "    print(\"--- Stemming ---\")\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Iterates over each element in the tokenlist\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokenlist]\n",
    "    print(\"Stemmed words:\")\n",
    "    print(stemmed_words)\n",
    "\n",
    "stem_text(tokenlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1M16ZMAfxr5"
   },
   "source": [
    "***LEMMATIZATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOg7LtjefZlW",
    "outputId": "efece819-8ab4-41c4-c211-bfdc1c96ad66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lemmatization ---\n",
      "Lemmatized Words:\n",
      "['Artificial', 'Intelligence', 'is', 'transforming', 'industry', 'worldwide', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform Lemmatization\n",
    "def lemmatize_text(words):\n",
    "    print(\"--- Lemmatization ---\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    print(\"Lemmatized Words:\")\n",
    "    print(lemmatized_words)\n",
    "\n",
    "lemmatize_text(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vA5XDMoTf2Fx"
   },
   "source": [
    "***REMOVE STOP WORDS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CR9s2wugeG5D",
    "outputId": "866f3f4a-d477-4d93-fb1a-96e9eaab7dbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stop Words ---\n",
      "Filtered Words (without stop words):\n",
      "['Artificial', 'Intelligence', 'transforming', 'industries', 'worldwide', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Identify and remove stop words\n",
    "def identify_stop_words(words):\n",
    "    print(\"--- Stop Words ---\")\n",
    "\n",
    "    # Retrieves a predefined list of stop words from the nltk.corpus.stopwords\n",
    "    # Module for the English language\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # COnverts the word to lowercase using word.lower()\n",
    "    # Check if the lowercase word is not in the stop_words set or not\n",
    "    # If the word is not a stop word, it is included in filtered_words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    print(\"Filtered Words (without stop words):\")\n",
    "    print(filtered_words)\n",
    "\n",
    "identify_stop_words(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXOytX03f6rH"
   },
   "source": [
    "***POS TAGGING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8lTpdieeHWw",
    "outputId": "fa6ca8b5-fb10-4c92-f1ab-78bf9098dbde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- POS Taggging ---\n",
      "POS Tags:\n",
      "[('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('is', 'VBZ'), ('transforming', 'VBG'), ('industries', 'NNS'), ('worldwide', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "def pos_tagging(words):\n",
    "    print(\"--- POS Taggging ---\")\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    print(\"POS Tags:\")\n",
    "    print(pos_tags)\n",
    "\n",
    "pos_tagging(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_aAQdHsgErw"
   },
   "source": [
    "***TEST PARSING AND BUILD PARSING TREE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "ENp2vZVSeJzR",
    "outputId": "8b106f84-1d44-493f-f4e7-9f447838f663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dependency Parsing ---\n",
      "Word: Artificial, Dependency: compound, Head: Intelligence, POS: PROPN\n",
      "Word: Intelligence, Dependency: nsubj, Head: transforming, POS: PROPN\n",
      "Word: is, Dependency: aux, Head: transforming, POS: AUX\n",
      "Word: transforming, Dependency: ROOT, Head: transforming, POS: VERB\n",
      "Word: industries, Dependency: dobj, Head: transforming, POS: NOUN\n",
      "Word: worldwide, Dependency: advmod, Head: transforming, POS: ADV\n",
      "Word: ., Dependency: punct, Head: transforming, POS: PUNCT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"478c4ff295134e918b29ff884586f739-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Artificial</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Intelligence</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">transforming</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">industries</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">worldwide.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-478c4ff295134e918b29ff884586f739-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-478c4ff295134e918b29ff884586f739-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-478c4ff295134e918b29ff884586f739-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-478c4ff295134e918b29ff884586f739-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-478c4ff295134e918b29ff884586f739-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-478c4ff295134e918b29ff884586f739-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-478c4ff295134e918b29ff884586f739-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-478c4ff295134e918b29ff884586f739-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-478c4ff295134e918b29ff884586f739-0-4\" stroke-width=\"2px\" d=\"M595,177.0 C595,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-478c4ff295134e918b29ff884586f739-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
    "\n",
    "# Perform dependency parsing\n",
    "def dependency_parsing(text):\n",
    "    print(\"--- Dependency Parsing ---\")\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        new_var = print(f\"Word: {token.text}, Dependency: {token.dep_}, Head: {token.head.text}, POS: {token.pos_}\")\n",
    "\n",
    "dependency_parsing(text)\n",
    "\n",
    "\n",
    "# To produce parsing true\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the ENglish language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input_text\n",
    "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
    "\n",
    "# Prcess the next\n",
    "doc = nlp(text)\n",
    "\n",
    "# Display the dependency tree in the notebook\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SciYb2ingRGp"
   },
   "source": [
    "***NAMED ENTITY RECOGNITION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VXmQoRteVjY"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Artificial Intelligence is transforming industries worldwide.\"\n",
    "\n",
    "# Perform Nmed Entity Recognition\n",
    "def named_entity_recognition(text):\n",
    "    print (\"--- Named Entity Recognition ---\")\n",
    "\n",
    "    # Pass input text to SpaCy LM(nlp) and return a doc onject.\n",
    "    # DOc object contains tokens, linguistic annotations, NE from the text.\n",
    "    doc = nlp(text)\n",
    "    print(doc)\n",
    "\n",
    "    # Iterates over all the named entities in the Doc object using doc.ents.\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Entity: {ent.text}, label: {ent.label_}\")\n",
    "\n",
    "named_entity_recognition(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGCSTNBTecP2"
   },
   "source": [
    "# **3. TASKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4p7RhROeYSG",
    "outputId": "ee658d55-3836-4703-eefd-1f1c5355c23f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available Texts in Gutenberg Corpus ---\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "\n",
      "--- FIrst 500 CHaracters of the text ---\n",
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died t\n",
      "\n",
      "--- Number of Sentence ---\n",
      "7493\n",
      "--- Number of Words ---\n",
      "191855\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Step 1: Load and Explore the Dataset\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"--- Available Texts in Gutenberg Corpus ---\")\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "# Select a text\n",
    "text = gutenberg.raw('austen-emma.txt')\n",
    "print(\"\\n--- FIrst 500 CHaracters of the text ---\")\n",
    "print(text[:500])\n",
    "\n",
    "# Step 2: Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"\\n--- Number of Sentence ---\")\n",
    "print(len(sentences))\n",
    "print(\"--- Number of Words ---\")\n",
    "print(len(words))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
